{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact-Checking Engine\n",
    "\n",
    "## Test data path\n",
    "\n",
    "This constant holds the path to a test data file. Change this path accordingly. We recommend placing the test data file in the data folder within the unzipped directory and using a relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"data/fokg-sw-test-2024.nt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/KG-mini/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, URIRef, RDF, Namespace\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.models import TransE\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "from pykeen.losses import MarginRankingLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the reference, training, and test knowledge graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Knowledge Graph length. 675859\n",
      "Training data length. 5000\n",
      "Test data length. 2000\n"
     ]
    }
   ],
   "source": [
    "# Load reference knowledge graph\n",
    "reference_kg = Graph()\n",
    "reference_kg.parse(\"data/reference-kg.nt\", format=\"nt\")\n",
    "print(\"Reference Knowledge Graph length.\", len(reference_kg))\n",
    "\n",
    "# Load training data\n",
    "train_graph = Graph()\n",
    "train_graph.parse(\"data/fokg-sw-train-2024.nt\", format=\"nt\")\n",
    "print(\"Training data length.\", len(train_graph))\n",
    "\n",
    "#Load test data\n",
    "test_graph = Graph()\n",
    "test_graph.parse(TEST_DATA_PATH, format=\"nt\")\n",
    "print(\"Test data length.\", len(test_graph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TransE model with reference_kg\n",
    "\n",
    "Prepare the reference triples from the reference knowledge graph. Train the TransE model to learn embeddings for entities and relations in the reference knowledge graph, which can later be used for downstream tasks like link prediction or fact-checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples in reference_kg:  660000\n"
     ]
    }
   ],
   "source": [
    "reference_triples = [(str(subj), str(pred), str(obj)) for subj, pred, obj in reference_kg if isinstance(subj, rdflib.URIRef) and isinstance(obj, rdflib.URIRef)] # only URIRef is considered\n",
    "print(\"Number of triples in reference_kg: \", len(reference_triples))\n",
    "reference_factory = TriplesFactory.from_labeled_triples(np.array(reference_triples, dtype=object))\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Train the TransE model on the reference knowledge graph.\n",
    "    \"\"\"\n",
    "    # TransE Model\n",
    "    embedding_dim = 200\n",
    "    margin = 1.0\n",
    "\n",
    "    model = TransE(\n",
    "        triples_factory=reference_factory,\n",
    "        embedding_dim=embedding_dim,\n",
    "        scoring_fct_norm=1,  # L1 distance\n",
    "        loss=MarginRankingLoss(margin=margin),  # margin-based ranking\n",
    "        random_seed= 42,\n",
    "    )\n",
    "\n",
    "    training_kg_loop = SLCWATrainingLoop(\n",
    "        model=model,\n",
    "        triples_factory=reference_factory,\n",
    "        optimizer=\"adam\",\n",
    "        optimizer_kwargs={\"lr\": 1e-3},\n",
    "        negative_sampler=\"basic\",\n",
    "        negative_sampler_kwargs={\"num_negs_per_pos\": 10},\n",
    "    )\n",
    "\n",
    "    num_epochs = 10          # can be reduced to 5 for faster computation\n",
    "    batch_size = 256\n",
    "    print(f\"TransE training parameters epochs={num_epochs}, batch_size={batch_size}\")\n",
    "    _ = training_kg_loop.train(\n",
    "        triples_factory=reference_factory,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "\n",
    "    print(\"TransE model trained with reference_kg.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Pretrained Model or Train a New One\n",
    "\n",
    "1. Faster Computation with Pretrained Model:\n",
    "If you want faster computation, the script will use the pretrained model (transE_model.pkl) if it exists in the same directory.\n",
    "\n",
    "2. Train the Model Yourself\n",
    "If you want to train the model from scratch, delete the \"transE_model.pkl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'transE_model.pkl' already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "model_path = \"transE_model.pkl\"\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"File '{model_path}' already exists. Skipping save.\")\n",
    "    # Load the model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "else:\n",
    "    model = train_model()\n",
    "    # Save the trained model using pickle\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding entity and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_id = reference_factory.entity_to_id\n",
    "relation_to_id = reference_factory.relation_to_id\n",
    "\n",
    "entity_representation = model.entity_representations[0]\n",
    "relation_representation = model.relation_representations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Train and Test data\n",
    "\n",
    "Extracts training and test data from RDF graphs using SPARQL queries. \n",
    "Training data includes triples and their truth values for supervised learning, while test data includes triples and their corresponding statement IRIs for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "query_train = prepareQuery(\"\"\"\n",
    "    SELECT ?stmt ?subject ?predicate ?object ?truthValue\n",
    "    WHERE {\n",
    "        ?stmt a <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?subject .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> ?predicate .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?object .\n",
    "        ?stmt <http://swc2017.aksw.org/hasTruthValue> ?truthValue .\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "train_triples = []\n",
    "train_truthValue = []\n",
    "for fact_iri, sub, pred, obj, truth_value in train_graph.query(query_train):\n",
    "    train_triples.append((sub.toPython(),pred.toPython(),obj.toPython()))\n",
    "    train_truthValue.append(truth_value.toPython())\n",
    "\n",
    "# Test Data\n",
    "query_test = prepareQuery(\"\"\"\n",
    "    SELECT ?stmt ?subject ?predicate ?object\n",
    "    WHERE {\n",
    "        ?stmt a <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?subject .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> ?predicate .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?object .\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "test_triples = []\n",
    "test_fact_iri = []\n",
    "for fact_iri, sub, pred, obj in test_graph.query(query_test):\n",
    "    test_triples.append((sub.toPython(),pred.toPython(),obj.toPython()))\n",
    "    test_fact_iri.append(fact_iri.toPython())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoded on embedded model\n",
    "\n",
    "Function `get_embedding_for_fact` to retrieve the embeddings for a given fact (subject, predicate, object) using a trained TransE model.\n",
    "It then generates embeddings for all training and test triples. The training embeddings (X_train) are paired with their corresponding truth values (y_train), while the test embeddings (X_test) are prepared for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_fact(subj, pred, obj):\n",
    "    if subj not in entity_to_id or obj not in entity_to_id or pred not in relation_to_id:\n",
    "        emb_dim = model.entity_representations[0]._embeddings.weight.shape[-1]\n",
    "        return np.zeros(3 * emb_dim)\n",
    "\n",
    "    s_id = entity_to_id[subj]\n",
    "    p_id = relation_to_id[pred]\n",
    "    o_id = entity_to_id[obj]\n",
    "\n",
    "    s_emb = model.entity_representations[0](indices=torch.tensor([s_id]))  # shape [1, dim]\n",
    "    p_emb = model.relation_representations[0](indices=torch.tensor([p_id]))\n",
    "    o_emb = model.entity_representations[0](indices=torch.tensor([o_id]))\n",
    "\n",
    "    cat = torch.cat([s_emb[0], p_emb[0], o_emb[0]], dim=0)\n",
    "    return cat.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "X_train = [get_embeddings_for_fact(s, p, o) for (s, p, o) in train_triples]\n",
    "y_train = np.array(train_truthValue)\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = [get_embeddings_for_fact(s, p, o) for (s, p, o) in test_triples]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train a MLPClassifier Model\n",
    "\n",
    "This code initializes and trains a Multi-Layer Perceptron (MLP) classifier with a specific architecture (three hidden layers with 256, 256, and 128 units), using the ReLU activation function and the Adam solver. It trains the model on the fact embeddings (X_train) and their corresponding truth values (y_train).\n",
    "The AUC (Area Under the ROC Curve) score is computed on the training data to evaluate model performance.\n",
    "The code also prepares the model to predict probabilities for the test data (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 256, 128), activation=\"relu\", solver=\"adam\", max_iter=50, random_state=42) # seed is fixed for reproducibility of result\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "train_probs = mlp.predict_proba(X_train)[:, 1]\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "print(f\"Train AUC: {train_auc:.4f}\")\n",
    "\n",
    "test_probs = mlp.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write it to result file\n",
    "\n",
    "Writes the test data results into a file (result.ttl).\n",
    "The lines have the following form:\n",
    "\n",
    "<http://dice-research.org/data/fb15k-237.ttl#3> <http://swc2017.aksw.org/hasTruthValue> \"5.11332036694511\"^^<http://www.w3.org/2001/XMLSchema#double> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.ttl\", \"w\") as resultFile:\n",
    "    for fact_iri, score in zip(test_fact_iri, test_probs):\n",
    "        line = f'<{fact_iri}> <http://swc2017.aksw.org/hasTruthValue> \"{score}\"^^<http://www.w3.org/2001/XMLSchema#double> .\\n'\n",
    "        resultFile.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG-mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
